---
title: Fundamentals of linear mixed models 
nav: Part 1
topics: Linear models review; Fixed effects vs. random effects
---

- [Welcome!](#welcome-)
- [Housekeeping](#housekeeping)
- [Outline for today](#outline-for-today)
- [Linear models review](#linear-models-review)
  * [The famous intercept-and-slope linear model](#the-famous-intercept-and-slope-linear-model)
  * [Let's fit the same statistical model using distribution notation and matrix notation](#let-s-fit-the-same-statistical-model-using-distribution-notation-and-matrix-notation)
  * [Whiteboard example - qualitative predictor](#whiteboard-example---qualitative-predictor)
- [Variance-covariance matrices](#variance-covariance-matrices)
  * [What does variance even mean?](#what-does-variance-even-mean-)
  * [On the covariance of two random variables $$y_1$$ and $$y_2$$](#on-the-covariance-of-two-random-variables---y-1---and---y-2--)
- [Adding a random effect to the model](#adding-a-random-effect-to-the-model)
  * [Independent observations](#independent-observations)
  * [Non-independent observations](#non-independent-observations)
  * [How do we define $$\beta_{0j}$$?](#how-do-we-define----beta--0j----)
    + [Fixed](#fixed)
    + [Random](#random)
- [Generalities -- what are mixed models anyways?](#generalities----what-are-mixed-models-anyways-)
  * [Random effects](#random-effects)
  * [Fixed effects versus random effects](#fixed-effects-versus-random-effects)
- [Applied example](#applied-example)
  * [Building the model](#building-the-model)
- [Wrap-up](#wrap-up)
- [What's next](#what-s-next)

------------------

## Welcome!

- About us:[Josefina](https://jlacasa.github.io/) and [Claudio](https://www.linkedin.com/in/claudio-dias-da-silva-jr-906432b9/).
- About you.
  - Frequent responses to the registration survey.   
  - Some knowledge of the *existence* of mixed effects models.  
  - Mostly life sciences - often heteroscedasticity and dependent observations!  

{% include figure.html img="day1/attendees.jpg" alt="Attendees counts" caption="Figure 1. Distribution of Departments attending this worksop" width="100%" id = "attendees" %}

## Housekeeping  

- We will have relatively low proportion of R code in this workshop. 
Instead, we will focus on the understanding of the components of mixed-effects models. 
Questions/concerns via e-mail are encouraged.   
- Emphasis on modeling and figuring out what mixed-effects models actually do.  
- The statistical notation we will use throughout this workshop is presented [here](0-prep). 
- DON'T PANIC when you see all the math notation and matrices. We will go slowly together. 
It is also **not expected** that you walk out of this workshop as a math notation wizard! 

{% capture text %}
1. Fundamentals of linear mixed models  
   Review of linear models & basic concepts of linear mixed models     
3. Modeling data generated by designed experiments
5. Generalized linear mixed models (aka non-normal response) applied to designed experiments  
{% endcapture %}
{% include card.html text=text header="Workshop Overview" %}


## Outline for today

-   **Review of linear models**: begin with a refresher on linear models (all fixed-effects), then introduce mixed-effects models by incorporating an additional assumption.
-   **The variance-covariance matrix**: the information of the random effects is stored in the variance-covariance matrix!  
-   **Fixed effects vs. random effects**: where/how is the information from each type of effect captured?
-   **Application**: practical implications of these concepts when fitting a model to your data.

------

## Linear models review  

### The famous intercept-and-slope linear model

{% include figure.html img="day1/linear_regression_1.jpg" alt="" caption="Figure 2. A good example for the intercept-and-slope model: apple diameter versus time." width="75%" id = "intercept_slope_fig1" %}

One of the most popular models is the intercept-and-slope model. 
It's so simple and interpretable! 
Most of us learned a way of writing out the statistical model called "model equation form". 
For a quantitative predictor $$x$$, the model equation form is    

$$y_{i} = \beta_0 + x_{i} \beta_1 + \varepsilon_{i}, \\ \varepsilon_i \sim N(0, \sigma^2),$$  

where $$y_{i}$$ is the observed value for the $$i$$th observation, 
$$\beta_0$$ is the intercept (i.e., the expected value of $$y$$ when $$x=0$$), 
$$\beta_1$$ is the slope parameter (i.e., the expected increase in $$y_i$$ with a unity increase in $$x$$), 
$$x_i$$ is the predictor for the $$i$$th observation, 
and $$\varepsilon_{i}$$ is the difference between the observed value $$y$$ 
and the expected value $$E(y_i) = \beta_0+x_i\beta_1$$ - that's why we often call it "residual". 
Typically, $$\boldsymbol{\beta} \equiv \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}$$ 
is estimated via maximum likelihood estimation. Also, when we assume a Normal distribution, 
maximum likelihood estimation yields the same point estimates as least squares estimation. 


Look at the plot above ([Figure 2](#intercept_slope_fig1)). 
The data are measurements of the diameter of apples that were **randomly selected from randomly selected trees** at different points in time.  

Can we fit the intercept-and-slope model to that data? 
Let's review the assumptions we make in this model (which is the default model in most software).   
- Linearity  
- Constant variance  
- Independence  
- Normality  


### Let's fit the same statistical model using distribution notation and matrix notation

There are other ways of writing out the statistical model above besides the model equation form. 
Instead of focusing on the distribution of the residuals, we can focus on the distribution of the data $$y$$:  

$$y_{i} \sim N(\mu_i, \sigma^2), \\ \mu_i = \beta_0 + x_{i} \beta_1.$$

This type of notation is called "probability distribution form". 
The probability distribution form makes it easier to switch to other distributions beyond the Normal ([Day 3](3-lesson) of this workshop). 
We can further express this equation using vectors and matrices:  

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma), \\ \boldsymbol{\mu} = \boldsymbol{1} \boldsymbol{\beta_0} + \mathbf{x} \boldsymbol{\beta_1} = \mathbf{X}\boldsymbol{\beta},$$

where $$n$$ is the total number of observations, $$p$$ is the total number of parameters, 
$$\mathbf{y}$$ is an $$n \times 1$$ vector containing all observations, 
$$\boldsymbol{\mu}$$ is an $$n \times 1$$ vector containing the expected values of said observations, 
$$\boldsymbol{\beta}$$ is an $$p \times 1$$ vector containing the (fixed) parameters, 
$$\mathbf{X}$$ is an $$n \times p$$ matrix containing the predictors, 
$$\Sigma$$ is an $$n \times n$$ matrix called variance-covariance matrix. 
*Note: This type of notation is also convenient to think about how to prepare the data in your spreadsheet. One row per observation (rows in $$\mathbf{X}$$), one variable per column (columns in $$\mathbf{X}$$).*

Let's expand these expressions: 

$$\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \sim  N \left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}, 
\begin{bmatrix} Cov(y_1, y_1) & Cov(y_1, y_2) & \dots & Cov(y_1, y_n) \\
Cov(y_2, y_1) & Cov(y_2, y_2) & \dots & Cov(y_2, y_n)\\
\vdots & \vdots & \ddots & \vdots \\ 
Cov(y_n, y_1) & Cov(y_n, y_2) & \dots & Cov(y_n, y_n) \end{bmatrix} \right),$$

which is the same as 

$$\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \sim N \left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}, 
\begin{bmatrix} Var(y_1) & Cov(y_1, y_2) & \dots & Cov(y_1, y_n) \\
Cov(y_2, y_1) & Var(y_2) & \dots & Cov(y_2, y_n)\\
\vdots & \vdots & \ddots & \vdots \\ 
Cov(y_n, y_1) & Cov(y_n, y_2) & \dots & Var(y_n) \end{bmatrix} \right).$$

### Whiteboard example - qualitative predictor  

Instead of a quantitative predictor, we could have a qualitative (or categorical) predictor. 
If we have two possible levels, A and B, we could use the same model as before,  

$$y_{i} \sim N(\mu_i, \sigma^2), \\ \mu_i = \beta_0 + x_i \beta_1,$$

and say that $$y_{i}$$ is the observed value for the $$i$$th observation, 
$$\beta_0$$ is the expected value for A, 
$$x_i = 0$$ if the $$i$$th observation belongs to A, and $$x_i = 1$$ if the $$i$$th observation belongs to B, 
$$\beta_1$$ is the difference between A and B. 

- What happens if the categorical predictor has more than two levels? 

## Variance-covariance matrices  

### What does variance even mean?  

Random variables are usually described with their properties like the expected value and variance. 
The expected value and variance are the first and second central moments of a distribution, respectively. 
Regardless of the distribution of a random variable $$Y$$, we could calculate its expected value $$E(Y)$$ and variance $$Var(Y)$$. 
The expected value measures the average outcome of $$Y$$. 
The variance measures the dispersion of $$Y$$, i.e. how far the possible outcomes are spread out from their average. 

{% include figure.html img="day1/normal_univariate.png" alt="Univariate Normal distributions" caption="Figure 3. Normal distributions" width="75%" id = "univariate_normal" %}

**Discuss in the plot above:**  
-   Expected value  
-   Variance  
-   Covariance?

### On the covariance of two random variables $$y_1$$ and $$y_2$$    

Covariance between two random variables means how the two random variables behave relative to each other. 
Essentially, it quantifies the relationship between their joint variability. 
The variance of a random variable is the covariance of a random variable with itself. 
Consider two variables $$y_1$$ and $$y_2$$ each with a variance of 1 and a covariance of 0.6. 
The data shown in [Figure 4](#multivariate_normal) arise from the multivariate normal distribution

$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} 10 \\ 8 \end{bmatrix} , \begin{bmatrix}1 & 0.6 \\ 0.6 & 1 \end{bmatrix} \right),$$

where the means of $$y_1$$ and $$y_2$$ are 10 and 8, respectively, and their covariance structure is represented in the variance-covariance matrix. Remember, 

$$\begin{bmatrix}y_1 \\ y_2 \end{bmatrix} \sim MVN \left( \begin{bmatrix} E(y_1) \\ E(y_2) \end{bmatrix} , \begin{bmatrix} Var(y_1) & Cov(y_1, y_2) \\ Cov(y_2,y_2) & Var(y_2) \end{bmatrix} \right).$$

{% include figure.html img="day1/normal_multivariate.jpg" alt="Multivariate Normal distribution" caption="Figure 4. Multivariate Normal distribution showing the correlation between two random normal variables." width="75%" id = "multivariate_normal" %}

**Discuss in the plot above:**  
-   Expected value  
-   Variance  
-   Covariance 

## Adding a random effect to the model   

### Independent observations  

Back to the example in [Figure 2](#intercept_slope_fig1). Let's assume we have $$n$$ observations of apple diameter. 
Remember, the apples were **randomly selected from random trees from a field**. 

If we used the default model in most software, we would assume  

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma),\\
\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ \vdots \\ y_n \end{bmatrix} \sim N
\left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4 \\ \vdots \\ \mu_n \end{bmatrix}, 
\sigma^2 
\begin{bmatrix} 1 & 0 & 0 & 0 & \dots & 0 \\ 
0 & 1 & 0 & 0 & \dots & 0 \\
0 & 0 & 1 & 0 & \dots & 0 \\
0 & 0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\  
0 & 0 & 0 & 0 & \dots & 1 \end{bmatrix}
\right),$$

which is the same as 

$$\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ \vdots \\ y_n \end{bmatrix} \sim N
\left( \begin{bmatrix}\mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4 \\ \vdots \\ \mu_n \end{bmatrix}, 
\begin{bmatrix} \sigma^2 & 0 & 0 & 0 & \dots & 0 \\ 
0 & \sigma^2 & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 & 0 & \dots & 0 \\
0 & 0 & 0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\  
0 & 0 & 0 & 0 & \dots & \sigma^2 \end{bmatrix}
\right).$$

Discuss the assumptions:
- Linearity  
- Constant variance  
- **Independence**  
- Normality  


### Non-independent observations  

Now, imagine that the observations are actually diameters from random apples, but they were taken from 5 different fields. 
These observations are no longer independent, because apples from the same field are more similar to each other than apples from different fields.
This is when mixed-effects models enter the story - they allow us to indicate *what is similar to what* via random effects. 
In this case, we expect the growth rate to be similar among fields, but the baseline (a.k.a., the intercept) to be field-specific. Then,   

$$y_{ij} = \beta_{0j} + x_{ij} \beta_1 + \varepsilon_{ij}, \\ \varepsilon_{ij} \sim N(0, \sigma^2),$$  


{% include modal.html button="Example data" color="success" title="Example data" 
text='<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fixed vs Random Effects Table</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
    </style>
</head>

<body>
    <table>
        <tr>
            <th>day</th>
            <th>diameter_cm</th>
            <th>field</th>
        </tr>
        <tr>
            <td>3</td>
            <td>2.9</td>
            <td>A</td>
        </tr>
        <tr>
            <td>3</td>
            <td>2.8</td>
            <td>B</td>
        </tr>
        <tr>
            <td>3</td>
            <td>2.9</td>
            <td>C</td>
        </tr>
        <tr>
            <td>3</td>
            <td>2.7</td>
            <td>D</td>
        </tr>
        <tr>
            <td>3</td>
            <td>3.0</td>
            <td>E</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.6</td>
            <td>A</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.9</td>
            <td>B</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.7</td>
            <td>C</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.8</td>
            <td>D</td>
        </tr>
        <tr>
            <td>6</td>
            <td>3.7</td>
            <td>E</td>
        </tr>
    </table>
</body>

These data would mean  

$$\mathbf{y} = \begin{bmatrix} 
2.9 \\
2.8 \\
2.9 \\
2.7 \\
3.0 \\
3.6 \\
3.9 \\
3.7 \\
3.8 \\
3.7 \end{bmatrix}$$.

For the all-fixed model, 
$$\mathbf{X} = \begin{bmatrix} 
1 & 3 & 1 & 0 & 0 & 0 & 0 \\
1 & 3 & 0 & 1 & 0 & 0 & 0 \\
1 & 3 & 0 & 0 & 1 & 0 & 0 \\
1 & 3 & 0 & 0 & 0 & 1 & 0 \\
1 & 3 & 0 & 0 & 0 & 0 & 1 \\
1 & 6 & 1 & 0 & 0 & 0 & 0 \\
1 & 6 & 0 & 1 & 0 & 0 & 0 \\
1 & 6 & 0 & 0 & 1 & 0 & 0 \\
1 & 6 & 0 & 0 & 0 & 1 & 0 \\
1 & 6 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}.$$

For the mixed model, 

$$\mathbf{X} = \begin{bmatrix} 
1 & 3 \\
1 & 3 \\
1 & 3 \\
1 & 3 \\
1 & 3 \\
1 & 6 \\
1 & 6 \\
1 & 6 \\
1 & 6 \\
1 & 6 \end{bmatrix},$$
$$\mathbf{Z} = \begin{bmatrix} 
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \end{bmatrix}.$$
' %}

### How do we define $$\beta_{0j}$$?

#### Fixed   

So far, we could have defined an all-fixed model. 

$$y_{ij} = \beta_{0j} + x_{ij} \beta_1 + \varepsilon_{ij}, \\ \beta_{0j} = \beta_0 + u_j \\ \varepsilon_{ij} \sim N(0, \sigma^2),$$  

where $$u_j$$ is the effect of the $$j$$th field on the intercept (i.e., on the baseline). 
In this case, $$u_j$$ is a fixed effect, which means it may be estimated via least squares estimation or maximum likelihood estimation. 
Under both least squares and maximum likelihood (assuming normal distribution), we may estimate the parameters by computing 

$$\hat{\boldsymbol{\beta}}_{ML} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y},$$

which yields the minimum variance unbiased estimate of $$\boldsymbol{\beta}$$. 

#### Random   

We could also assume that the effects of the $$j$$th tree (i.e., $$b_j$$) arise from a random distribution. 
The most common assumption (and the default in most statistical software) is that 

$$u_j \sim N(0, \sigma^2_b).$$

Now, we don't estimate the effect, but the variance $$\sigma^2_b$$. 
Note that there are $$J$$ levels of the random effects, meaning they are **categorical**.  
Also, now 

$$\hat{\boldsymbol{\beta}}_{REML} = (\mathbf{X}^T \mathbf{V}^{-1} \mathbf{X})^{-1}\mathbf{X}^T \mathbf{V}^{-1} \mathbf{y},$$

where $$\mathbf{V} = Var(\mathbf{y})$$ is the variance-covariance matrix of $$\mathbf{y}$$, 
including residual variance and random-effects variance. Note that this formula yields 
the same point estimate for $$\boldsymbol{\beta}$$, but with a different confidence interval.  


## Generalities -- what are mixed models anyways?

Mixed models combine fixed effects and random effects. 
Generally speaking, we can write out a mixed-effects model using the model equation form, as   

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\mathbf{u} + \boldsymbol{\varepsilon}, \\ 
\begin{bmatrix}\mathbf{u} \\ \boldsymbol{\varepsilon} \end{bmatrix} \sim \left(
\begin{bmatrix}\boldsymbol{0} \\ \boldsymbol{0} \end{bmatrix}, 
\begin{bmatrix}\mathbf{G} & \boldsymbol{0} \\
\boldsymbol{0} & \mathbf{R} \end{bmatrix} 
\right),$$

where $$\mathbf{y}$$ is the observed response, 
$$\mathbf{X}$$ is the matrix with the explanatory variables, 
$$\mathbf{Z}$$ is the design matrix,
$$\boldsymbol{\beta}$$ is the vector containing the fixed-effects, 
$$\mathbf{u}$$ is the vector containing the random effects, 
$$\boldsymbol{\varepsilon}$$ is the vector containing the residuals, 
$$\mathbf{G}$$ is the variance-covariance matrix of the random effects, 
and $$\mathbf{R}$$ is the variance-covariance matrix of the residuals. 
Note that $$\mathbf{X} \boldsymbol{\beta}$$ is the fixed effects part of the model, and 
$$\mathbf{Z}\mathbf{u}$$ is the random effects part of the model.



{% include modal.html button="Example for <strong>X</strong> and <strong>Z</strong>" color="success" 
title="Example for <strong>X</strong> and <strong>Z</strong>" 
text="<strong>Example A.</strong> Let's focus on the first 10 observations of apple diameter. 
Said first 10 observations of apple diameters include days 3 and 6 (which you can find in <strong>X</strong>), and one observation per field for each day (which you can find in <strong>Z</strong>). $$\mathbf{X} = \begin{array}{cc}  
\text{Int} \phantom{-} \text{day} \\ 
\begin{bmatrix} 
1 & 3 \\
1 & 6 \\
1 & 3 \\
1 & 6 \\
1 & 3 \\
1 & 6 \\
1 & 3 \\
1 & 6 \\
1 & 3 \\
1 & 6 
\end{bmatrix} 
\end{array}$$, $$\mathbf{Z} = \begin{array}{cc}
\text{f}1 \phantom{-} \text{f}2 \phantom{-} \text{f}3 \phantom{-} \text{f}4 \phantom{-} \text{f}5 \\ 
\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 \end{bmatrix}
\end{array}$$

------

<strong>Example B.</strong> Let's focus on the first 10 observations of apple diameter. 
In this case, we aim to predict <strong>final</strong> diameter based on the apple variety: Red delicious (RD), Gala (G) or Fuji (F).
You can still find this information in <strong>X</strong>, . The <strong>Z</strong> matrix remains the same. 

$$\begin{array}{ccc}  
& \text{RD} \phantom{-} \text{G} \phantom{-} \text{F} \\ 
\mathbf{X} = &
\begin{bmatrix} 
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 
\end{bmatrix} 
\end{array}$$, 
$$\mathbf{Z} = \begin{array}{cc}
\text{f}1 \phantom{-} \text{f}2 \phantom{-} \text{f}3 \phantom{-} \text{f}4 \phantom{-} \text{f}5 \\ 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \end{bmatrix}
\end{array}$$
" %}

Using the probability distribution form, we can then say that $$E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$$ 
and $$Var(\mathbf{y}) = \mathbf{V} = \mathbf{Z}\mathbf{G}\mathbf{Z}' + \mathbf{R}$$. 
Usually, we assume $$\mathbf{G} = \sigma^2_u 
\begin{bmatrix} 1 & 0 & 0 & \dots 0 \\
0 & 1 & 0 & \dots 0 \\
0 & 0 & 1 & \dots 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots 1
\end{bmatrix} $$ and  $$\mathbf{R} = \sigma^2 
\begin{bmatrix} 1 & 0 & 0 & \dots 0 \\
0 & 1 & 0 & \dots 0 \\
0 & 0 & 1 & \dots 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots 1
\end{bmatrix} $$. 

Then,   

$$\mathbf{y} \sim N(\boldsymbol{\mu}, \Sigma),$$  

$$\Sigma = \begin{bmatrix} \sigma^2 + \sigma^2_u & \sigma^2_u & 0 & 0 & 0 & 0 &\dots & 0\\
\sigma^2_u & \sigma^2 + \sigma^2_u & 0 & 0 & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 + \sigma^2_u & \sigma^2_u  & 0 & 0 & \dots & 0 \\
0 & 0 & \sigma^2_u & \sigma^2 + \sigma^2_u  & 0 & 0 & \dots & 0 \\
0 & 0 & 0 & 0 & \sigma^2 + \sigma^2_u & \sigma^2_u  & \dots & 0 \\
0 & 0 & 0 & 0 & \sigma^2_u & \sigma^2 + \sigma^2_u  & \dots & \vdots \\


\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & 0 & 0 & 0 & \dots & \sigma^2 + \sigma^2_u
\end{bmatrix}.$$

Take your time to digest the variance-covariance matrix above. What type of data do you think generated it? 

### Random effects    

- By definition, random effects are regression coefficients that arise from a random distribution. 
- Typically, a random effect $$u \sim N(0, \sigma^2_u)$$.   
- We estimate the variance $$\sigma^2_u$$.  
- Calculating degrees of freedom can get much more complex than in all-fixed effects models (e.g., with unbalanced data, spatio-temporally correlated data, or non-normal data).  
- In the context of designed experiments, random effects are assumed to be independent to each other and independent to the residual.  


**Estimation**  

Restricted maximum likelihood estimation (REML) is the default in most mixed effects models because, for small data (aka most experimental data), maximum likelihood (ML) provides variance estimates that are downward biased.
- In REML, the likelihood is maximized after accounting for the model’s fixed effects.  

- In ML, $$\ell_{ML}(\boldsymbol{\sigma; \boldsymbol{\beta}, \mathbf{y}}) = - (\frac{n}{2}) \log(2\pi)-(\frac{1}{2}) \log ( \vert \mathbf{V}(\boldsymbol\sigma) \vert ) - (\frac{1}{2}) (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^T[\mathbf{V}(\boldsymbol\sigma)]^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})$$  
- In REML, $$\ell_{REML}(\boldsymbol{\sigma};\mathbf{y}) = - (\frac{n-p}{2}) \log (2\pi) - (\frac{1}{2}) \log ( \vert \mathbf{V}(\boldsymbol\sigma) \vert ) - (\frac{1}{2})log \left(  \vert \mathbf{X}^T[\mathbf{V}(\boldsymbol\sigma)]^{-1}\mathbf{X} \vert \right) - (\frac{1}{2})\mathbf{r}[\mathbf{V}(\boldsymbol\sigma)]^{-1}\mathbf{r}$$, 
where $$p = rank(\mathbf{X})$$, $$\mathbf{r} = \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}_{ML}$$.  
  - Start with initial values for $$\boldsymbol{\sigma}$$, $$\tilde{\boldsymbol{\sigma}}$$.  
  - Compute $$\mathbf{G}(\tilde{\boldsymbol{\sigma}})$$ and $$\mathbf{R}(\tilde{\boldsymbol{\sigma}})$$.  
  - Obtain $$\boldsymbol{\beta}$$ and $$\mathbf{b}$$.   
  - Update $$\tilde{\boldsymbol{\sigma}}$$.  
  - Repeat until convergence.  


### Fixed effects versus random effects  

**Group discussion:** what determines if an effect should be random of fixed? 
Consider the assumptions:  

- $$\hat{\boldsymbol{\beta}} \sim N \left( \boldsymbol{\beta}, (\mathbf{X}^T \mathbf{V}^{-1} \mathbf{X})^{-1} \right) $$ 
- $$u_j \sim N(0, \sigma^2_u)$$ 
- What process is being studied?  
- How were the levels selected? (randomly, carefully selected)  
- How many levels does the factor have, vs. how many did we observe?   

Some good references:  
- Page 20 in Gelman (2005). "Analysis of variance—why it is more important than ever". [[link](https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-1/Analysis-of-variancewhy-it-is-more-important-than-ever/10.1214/009053604000001048.full)]


## Applied example  

-   Field experiment at Colby, KS.  
-   One treatment factor: genotype (treatment structure).  
-   Randomized Complete Block Design with 3 repetitions (design structure).  


{% include figure.html img="day1/emmeans_blocks.jpg" alt="" caption="Figure 5. Designed experiment. Colors indicate different genotypes." width="50%" id = "applied_ex" %}

### Building the model  

1. What is the *experiment blueprint*? (aka design structure)  
2. What are the treatments?  

We can easily come up with two models:

1.  Blocks fixed $$y_{ijk} = \mu + \tau_i + \rho_j + \varepsilon_{ijk}; \ \ \varepsilon \sim N(0, \sigma^2)$$.  
2.  Blocks random $$y_{ijk} = \mu + \tau_i + u_j + \varepsilon_{ijk}; \ \ u_j \sim N(0, \sigma^2_u); \ \ \varepsilon \sim N(0, \sigma^2)$$, 
where $$u$$ and $$\varepsilon$$ are independent.

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embed R Code</title>
    <style>
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow-x: auto; /* Enables horizontal scrolling if the code is too wide */
        }
    </style>
</head>
<body>
    <pre>
<code>
library(lme4)
library(tidyverse)
library(emmeans)
library(latex2exp)

df <- read.csv("cochrancox_kfert.csv")
df$rep <- as.factor(df$rep)
df$K2O_lbac <- as.factor(df$K2O_lbac)
m_fixed <- lm(yield ~ K2O_lbac + rep, data = df)
m_random <- lmer(yield ~ K2O_lbac + (1|rep), data = df)

(mg_means_fixed <- emmeans(m_fixed, ~K2O_lbac, contr = list(c(1, 0, 0, 0, -1))))
</code>
    </pre>
</body>
</html>

{% highlight text %}
## $emmeans
##  K2O_lbac emmean    SE df lower.CL upper.CL
##  36         7.92 0.121  8     7.64     8.19
##  54         8.12 0.121  8     7.84     8.40
##  72         7.81 0.121  8     7.53     8.09
##  108        7.58 0.121  8     7.30     7.86
##  144        7.52 0.121  8     7.24     7.79
## 
## Results are averaged over the levels of: rep 
## Confidence level used: 0.95 
## 
## $contrasts
##  contrast          estimate    SE df t.ratio p.value
##  c(1, 0, 0, 0, -1)      0.4 0.171  8   2.344  0.0471
## 
## Results are averaged over the levels of: rep
{% endhighlight %}


{% highlight r %}
(mg_means_random <- emmeans(m_random, ~K2O_lbac, contr = list(c(1, 0, 0, 0, -1))))
{% endhighlight %}



{% highlight text %}
## $emmeans
##  K2O_lbac emmean    SE   df lower.CL upper.CL
##  36         7.92 0.162 5.57     7.51     8.32
##  54         8.12 0.162 5.57     7.72     8.52
##  72         7.81 0.162 5.57     7.41     8.21
##  108        7.58 0.162 5.57     7.18     7.98
##  144        7.52 0.162 5.57     7.11     7.92
## 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95 
## 
## $contrasts
##  contrast          estimate    SE df t.ratio p.value
##  c(1, 0, 0, 0, -1)      0.4 0.171  8   2.344  0.0471
## 
## Degrees-of-freedom method: kenward-roger
{% endhighlight %}

------

## Wrap-up  

Now we know what we mean when we say "factor A was considered fixed and factor B was considered random"!  

**Group discussion:** What to write in the Materials and Methods section of a paper. 
- Field-specific consensus  
- Enough to be reproducible 


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fixed vs Random Effects Table</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
    </style>
</head>

<body>

<table>
    <tr>
        <th> </th>
        <th>Fixed effects</th>
        <th>Random effects</th>
    </tr>
    <tr>
        <th>Where</th>
        <td>Expected value</td>
        <td>Variance-covariance matrix</td>
    </tr>
    <tr>
        <th>Inference</th>
        <td>Constant for all groups in the population of study</td>
        <td>Differ from group to group</td>
    </tr>
    <tr>
        <th>Usually used to model</th>
        <td>Carefully selected treatments or genotypes</td>
        <td>The study design (aka structure in the data, or what is similar to what)</td>
    </tr>
    <tr>
        <th>Assumptions</th>
        <td>$$\hat{\boldsymbol{\beta}} \sim N \left( \boldsymbol{\beta}, (\mathbf{X}^T \mathbf{V}^{-1} \mathbf{X})^{-1} \right) $$</td>
        <td>$$u_j \sim N(0, \sigma^2_u)$$</td>
    </tr>
    <tr>
        <th>Method of estimation</th>
        <td>Maximum likelihood, least squares</td>
        <td>Restricted maximum likelihood (shrinkage)</td>
    </tr>
</table>
</body>

## What's next  

- Friday, same time, same place.  
- More applied examples and what they mean.  
- Some troubleshooting.  

Any questions? E-mail me!  


