---
title: 'Day 3: GLMMs'
author: "Claudio D. Silva Jr"
output: html_document
---

# Day 3: Generalized Linear Mixed Models - GLMMs

Learning objectives:

-   What are GLMMs

-   What is a distributional assumption

-   How to fit a GLMM

$$
\\[0.2in]
$$

## What are GLMMs

-   Generalized Linear Models are models in which we can assume different distributions for our data beyond the Normal distribution.

-   Similar to general linear models, GLMs can also have random effects, thus, Generalized Linear Mixed Models - GLMMs.

$$
\\[0.2in]
$$

### The structure of a GLMMs

Remember that for a **LMMs**, assuming $\mathbf{y}$ arises from a normal distribution, we have:

$$
\mathbf{y = X\beta + Zu + \epsilon} \\ \mathbf{\begin{bmatrix} u \\ \epsilon \end{bmatrix} \sim \begin{pmatrix}  \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} G \; 0 \\ 0 \; R \end{bmatrix} \end{pmatrix}}
$$

In which:

-   $\mathbf{X\beta}$ represents our fixed effects, where $\mathbf{X}$ is a matrix containing our explanatory variables and $\mathbf{\beta}$ a vector containing the fixed-effects parameters.

-   $\mathbf{Zu}$ represents our random effects, where $\mathbf{Z}$ is a design matrix and $\mathbf{u}$ is the vector containing the random effects parameters.

-   $\mathbf{\epsilon}$ is the vector containing the residuals.

-   From $\mathbf{Zu + \epsilon}$ we have: $\mathbf{G}$ is the variance-covariance matrix of the random effects, and $\mathbf{R}$ is the variance-covariance matrix of the residuals.

    -   $\mathbf{G = \sigma^2_uI}$ and $\mathbf{R = \sigma^2I}$, in which $\mathbf{I}$ is the identity matrix.

Which is similar to:

$$
\mathbf{u} \sim N(0, I\sigma^2_u) \\
\mathbf{\epsilon} \sim N(0, I\sigma^2)
$$

In this case:

$$
E(\mathbf{y}) = \mathbf{X\beta}, \\
Var(\mathbf{y}) = \mathbf{ZGZ' + R}
$$

We can also write this model as:

$$
\mathbf{y} \sim N(\mathbf{X\beta}, \; \mathbf{ZGZ' + R})
$$

or:

$$
\mathbf{y} \sim N(\mathbf{X\beta}, \; \mathbf{\Sigma}) \\
\mathbf{\Sigma} = \mathbf{ZGZ' + R}
$$

For **GLMMs** the structure changes based on the distribution we will assume for $\mathbf{y}$, but is very similar to the last notation presented. A generic definition would be:

$$
\mathbf{y|u} \sim P(\mu, \; \phi)
$$

In which:

-   Linear predictor: $g(\mu) = \eta = \mathbf{X\beta + Zu}$

    -   $g(\mu) = \eta$ is the link function applied to the expected value.

    -   $E(\mathbf{y|u}) = \mu$.

$$
\\[0.2in]
$$

### Components of GLMMs

#### Link Functions

Our linear predictor $\mathbf{X\beta}$ can produce all possible values in the y-axis of a plot, from $- \; \infty$ to $+ \; \infty$ depending on the value of the predictor variable. A link function links the linear predictor and the distribution assumed for the data $\mathbf{y}$.

In the **link scale**, the mean of $\mathbf{y}$ respect linearity of the linear predictor. In the **response scale**, the mean $\mathbf{\mu}$ is back transformed by the inverse link and respects the support of the distribution.

The link function is applied to the expected value ($E(\mathbf{y})$), and not to the observations. Transformation of the observations also effect the error, while link functions only affect the parameters controlling the expected value.

Example of link functions:

| Link Function | Equation | Use | Why |
|:----------------:|:----------------:|:----------------:|:----------------:|
| **Identity Link** | $g(\mu) = \mu$ | Normal dist. | $E(\mathbf{y})$ can take any real value ($-\infty, \; +\infty$) |
| **Logit Link** | $g(\mathbf{\mu}) = log(\frac{\mu}{1-\mu})$ | Logistic, Beta, Binomial dist. | $E(\mathbf{y})$ can take any values between 0 and 1. Maps $(0, \; 1) \rightarrow (-\infty, \; +\infty)$ |
| **Log Link** | $g(\mu) = log(\mu)$ | Poisson, Gamma dist. | $E(\mathbf{y})$ can take any positive values ($\mu > 0$) |

$$
\\[0.2in]
$$

#### Distributional assumption for the data

GLMMs support different distributions from the exponential family. Distributions from the exponential family share common structure, but are relatively different among themselves.

-   **Assumption**: Something you take as true about your data or about the process that generated it!

Important distributions to know are:

-   **For continuous data**: Normal, t, Gamma, Beta.

-   **For discrete data**: Binomial, Poisson, Negative Binomial.

$$
\\[0.2in]
$$

<center>

#### **Normal distribution**

</center>

$$
y \sim N(\mu, \; \sigma^2)
$$

$$
E(y) = \mu \\
Var(y) = \sigma^2
$$

**Support:**

$$
y \in (-\infty, \; +\infty)
$$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
x <- seq(-10, 10, 0.1) 
plot(x, dnorm(x, 0, 1), type = "l", lwd = 2, ylab = "[x]", xlab = "x", ylim = c(0, 0.4)) 
lines(x, dnorm(x, 0, 4), lty = 2) 
lines(x, dnorm(x, 2, 1), lty = 3) 
legend("topright", legend = c("x ~ Normal(0,1)", "x ~ Normal(0,16)", "x ~ Normal(2, 1)"), lty = c(1,2,3), bty = "n")
```

<center>

#### **Student t distribution**

</center>

$$
y\sim t_v(\mu, \; \sigma^2)
$$

<center>

$E(y) = \mu$ for $v > 1$, otherwise undefined

$Var(y) = \frac{v}{v-2} \sigma^2$, otherwise undefined

</center>

**Support:**

$$
y \in (-\infty, \; +\infty)
$$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
x <- seq(-10, 10, 0.1) 
plot(x, dt(x, 2), type = "l", lwd = 2, ylab = "[x]", xlab = "x", ylim = c(0, 0.4)) 
lines(x, dt(x, 100), lty = 2) 
lines(x, dt(x, 2, 2), lty = 3) 
legend("topright", legend = c("x ~ t(2)", "x ~ t(100)", "x ~ t(2, 2)"), lty = c(1,2,3), bty = "n")
```

<center>

#### **Gamma distribution**

</center>

$$
y \sim Gamma(\alpha, \; \beta)
$$

$$
E(y) = \frac{\alpha}{\beta} \\
Var(y) = \frac{\alpha}{\beta^2}
$$

**Support:**

$$
y \in (0, \; +\infty)
$$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
x <- seq(0, 12.5, 0.1) 
plot(x, dgamma(x, 2, 2), type = "l", lwd = 2, ylab = "[x]", xlab = "x", ylim = c(0, 1.5)) 
lines(x, dgamma(x, 10, 2), lty = 2) 
lines(x, dgamma(x, 3, 5), lty = 3) 
legend("topright", legend = c("x ~ Gamma(2, 2)", "x ~ Gamma(10, 2)", "x ~ Gamma(3, 5)"), lty = c(1,2,3), bty = "n")
```

<center>

#### **Beta distribution**

</center>

$$
y \sim Beta(\alpha, \; \beta)
$$

$$
E(y) = \frac{\alpha}{\alpha + \beta} \\
Var(y) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$

**Support**:

$$
y \in (0, \; 1)
$$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
x <- seq(0.01, 0.99, 0.01) 
plot(x, dbeta(x, 0.5, 0.5), type = "l", lwd = 2, ylab = "[x]", xlab = "x", ylim = c(0, 3)) 
lines(x, dbeta(x, 1, 1), lty = 2) 
lines(x, dbeta(x, 4, 2), lty = 3) 
legend("top", legend = c("x ~ Beta(0.5, 0.5)", "x ~ Beta(1, 1)", "x ~ Beta(4, 2)"), lty = c(1,2,3), bty = "n")
```

<center>

#### **Poisson distribution**

</center>

$$
y \sim Poisson(\lambda)
$$

$$
E(y) = \lambda \\
Var(y) = \lambda
$$

**Support:**

$$
y \in (0, 1, 2, ..., +\infty)
$$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
x <- seq(0, 17, 1)
off.set <- 0.2
plot(x, dpois(x, 2), type = "h", lwd = 6, ylab = "[x]", xlab = "x", ylim = c(0, 0.3), col = adjustcolor("#C3C3C3", alpha.f = 1)) 
lines(x + off.set, dpois(x, 6), type = "h", lwd = 6, col = adjustcolor("#757575", alpha.f = 1)) 
lines(x + 2*off.set, dpois(x, 10), type = "h", lwd = 6, col = adjustcolor("#1E1E1E", alpha.f = 1))  
legend("topright", legend = c("x ~ Pois(2)", "x ~ Pois(6)", "x ~ Pois(10)"), lty = c(1, 1, 1), lwd = 6, col = c("#C3C3C3", "#757575", "#1E1E1E"), bty = "n")
```

$$
\\[0.2 in]
$$

## Working with GLMMs

1.  Define a distribution that matches $y$.
2.  Define the linear predictor (fixed and random effects) $\eta$.
3.  Define the link function that connects $E(y)$ of the assume distribution and the linear predictor $\eta$.

$$
\\[0.2 in]
$$

## Example I

In this example we will evaluate two different responses, yield and disease severity. The data was generated from a designed experiment on fungicide efficacy to manage the disease know and yellow rust on wheat. The experimental design was a Randomized Complete Block Design (RCBD).

### Yield

1.  Define a distribution that matches $y$.

$$
y \sim N(\mu, \; \sigma^2)
$$

-   Why?

    -   Yields cannot be negative, but usually follow the normal bell-shaped pattern.

2.  Define a linear predictor $\eta$

$$
\eta_{ij} = \mu_0 + t_i + u_j
$$

-   Where:

    -   $\mu_0$ represents the overall/gran mean.
    -   $t_i$ is the parameter for the effect of treatment, in this case, fungicides - **Fixed effect.**
    -   $u_j$ is the parameter for the effect of block - **Random effect.**

3.  Define the link function that connects $E(y)$ of the assume distribution and the linear predictor $\eta$.

<center>**Identity link**</center>

$$
g(\mu) = \eta = \mu
$$

-   Why?

    -   The linear predictor can produce values in the y-axis that range from $-\infty$ to $+\infty$, which matches the support of the assumed normal distribution, not requiring adjustments.

    -   Remember the support of the normal distribution:

$$
y \in (-\infty, \; +\infty)
$$

#### **Analysis of yield**

**Model**

$$
y_{ij}|u_j \sim N(\mu_{ij}, \; \sigma^2) \\ \mu_{ij} = \eta_{ij} = \mu_0 + t_i + u_j \\ u_j \sim N(0, \; \sigma^2_u)
$$

**Data**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(readxl)
library(ggplot2)
library(emmeans)
df <- read_excel("C:/Users/cdiasda/OneDrive - Kansas State University/Desktop/Doutorado/Laboratory and research/Workshops/Mixed Models Workshop/USA041.xlsx")
df$trt <- as.factor(df$trt)
df$rep <- as.factor(df$rep)

d1 <- df[,c(1, 2, 34)]
colnames(d1) <- c("fungicide", "block", "yield")
```

```{r, warning=FALSE, message=FALSE}
print(d1)
```

**Fitting the model**

```{r, warning=FALSE, message=FALSE}
library(glmmTMB)
m1 <- glmmTMB(yield ~ fungicide + (1|block), data = d1)
summary(m1)
```

**Checking the model - Residuals**

```{r, warning=FALSE, message=FALSE}
res <- residuals(m1, type = "pearson")
fit_link <- predict(m1, type = "link")

## QQ Plot
qqnorm(res)
qqline(res, col="red", lwd=2)

## Residual vs Predicted
plot(fit_link, res, ylab = "Residuals", xlab = "Predicted")
```

**What are we checking here?**

Residuals: "The footprint of the fitted model" - How much model predictions deviate from observations.

$$
\epsilon_i = y_i - \hat{y}_i
$$

Pearson residuals that we used here: Difference divided by the standard deviation.

$$
\epsilon_i = \frac{y_i - \hat{y}_i}{sd(y_i)}
$$

What is the idea behind the QQ-Plot and the residual scatter plot?

-   QQ-Plot: Compare the quantiles[^1] of two distributions, if they are similar, we expect a straight diagonal line. In this case, in the y-axis we have the quantiles of our residuals and in the x-axis the quantiles of a standard normal distribution.

-   Residual vs Predict plot: Show whether we have homocedasticity of the variances (constant variance across predicted values) or not. Usually, if higher or lower predicted values have higher or lower variance, the plot will present a "funnel" shape. Ideally the plot is a random scatter of points.

-   Reminder: Here, assuming Normal distribution, we are checking the same assumption of a common linear regression:

[^1]: Quantiles divide a dataset into equal-sized subsets, helping to understand more about the distribution of the data.

$$
\epsilon_i \sim i.i.d.N(0, \sigma^2)
$$

**Important note:** Here we are looking at conditional residuals (within block cluster), which includes the random effects. We compare each observation to its expected value given both fixed and random effects ($\overline{y}_{i.}|u_j \sim N(\mu_0 + t_i + u_j, \; \sigma^2)$).

**ANOVA**

```{r, warning=FALSE, message=FALSE}
library(car)
Anova(m1)
```

**Post-hoc test - Mean comparisons**

```{r, warning=FALSE, message=FALSE}
library(emmeans)
library(multcomp)
means <- emmeans(m1, ~ fungicide)
cld(means, Letters = letters)
```

$$
\\[0.2 in]
$$

### Severity

Severity refers to how much an specific plant organ is affected by a given disease. In this case it refers to the leaf area covered by yellow rust lesions, also know as pustules.

1.  Define a distribution that matches $y$.

$$ y \sim Beta(\mu, \; \phi) $$

-   Why?

    -   Severity is a percentage: 0 - 100%, in proportion: 0 - 1.

    -   The support from the Beta distribution perfectly matches our response.

    -   Remember the support for the Beta distribution:

$$
y \in (0, \; 1)
$$

2.  Define a linear predictor $\eta$.

$$ \eta_{ij} = \mu_0 + t_i + u_j $$

-   Where:

    -   $\mu_0$ represents the overall/gran mean.
    -   $t_i$ is the parameter for the effect of treatment, in this case, fungicides - **Fixed effect.**
    -   $u_j$ is the parameter for the effect of block - **Random effect.**

3.  Define the link function that connects $E(y)$ of the assume distribution and the linear predictor $\eta$.

<center>**Logit link**</center>

$$ g(\mu) = \eta = logit(\mu)$$

-   Why?

    -   Logit links $(-\infty, \; +\infty)$ to $(0, \; 1)$, that is our desired scale.

#### **Analysis of severity**

**Model**

$$
y_{ij}|u_j \sim Beta(logit(\mu_{ij}), \; \phi) \\ logit(\mu_{ij}) = \eta_{ij} = \mu_0 + t_i + u_j \\ u_j \sim N(0, \sigma^2_u)
$$

**Data**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
d2 <- df[,c(1, 2, 18)]
colnames(d2) <- c("fungicide", "block", "severity_o")
d2$severity <- d2$severity_o/100
```

```{r, warning=FALSE, message=FALSE}
print(d2)
```

**Fitting the model**

```{r, warning=FALSE, message=FALSE}
m2 <- glmmTMB(severity ~ fungicide + (1|block), family = beta_family(link = "logit"), data = d2)
summary(m2)
```

**Checking the model - Residuals**

```{r, warning=FALSE, message=FALSE, eval=FALSE, echo=FALSE}
res2 <- residuals(m2, type = "pearson")
fit_link2 <- predict(m2, type = "link")

## QQ Plot
qqnorm(res2)
qqline(res2, col="red", lwd=2)

## Heterogeneity
plot(fit_link2, res2, ylab = "Residuals", xlab = "Predicted")
```

```{r, warning=FALSE, message=FALSE}
library(DHARMa)
res_sim2 <- simulateResiduals(m2, plot = TRUE)
```

**What are we checking here?**

For linear mixed models our residuals are assumed to be normally distributed and with constant variance ($i.i.d. Normal$). In models with different distributional assumptions, residuals do not follow a known distribution, which makes it tricky to check models in a similar way we did before, it does not make much sense to test normality and homocedasticity here.

**What is each test doing?**

-   QQ-Plot

    -   Kolmogorov-Smirnov test: Test for uniformity against a uniform distribution - $Uniform(0, \; 1)$.

    -   Dispersion test: Variance in the observations vs. Variance on the simulations.

    -   Outlier test: Residual values of 0 or 1. Test if the number of outliers is appropriate to the size of the data. Does not quantify the amount of outliers.

-   Residual vs Predicted

    -   Similar to before, we expect to see scatter points across predictions, with no trend.

    -   Fit smoothed splines in three points of the quantile residuals, 0.25, 0.50, and 0.75. Test whether these lines are flat or have trends. For a random scatter, we do not expect to see trends at any of these points, this would indicate heterocedasticity.

    -   It also indicated when outliers are detected by producing a red asterisk.

**We can use simulated residuals - DHARMa (Residual Diagnostics for HierArchical Regression Models) package**

What is DHARMa doing?

1.   Simulate "new observations" (predictions) using fitted values ($\hat{\mu}$ and $\hat{\phi}$) as the model distribution parameters (simulate many "fake" data):

$$
y_i \sim Beta(\hat{\mu}_i, \; \hat{\phi})
$$

2.  Calculate the quantile for the cumulative distribution function (CDF)[^2]:

[^2]: Cumulative distribution function (CDF) tells the probability that a random variable takes a value less than or equal $y$. How much of the mass lies bellow that point for a given distribution with defined parameters.

$$
r_{i, q} \sim CDF(y_i; \; \hat{\mu}, \; \hat{\phi})
$$

-   The quantiles are our residuals here.

3.  We check the residuals for correct model specification, we expect to see values between 0 and 1:

-   0 every simulated value is larger than $y_i$.

-   1 every simulated value is smaller or equal $y_i$.

-   0.5 $y_i$ is right in the middle of the simulated values.

We expect that our residuals will look like an Uniform distribution: $r_{i,q} \sim Uniform(0, 1)$.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
x <- seq(0, 1, by = 0.1)
hist(dunif(x, 0, 1), ylab = "[x]", xlab = "x", main = "Uniform(0, 1)", freq = FALSE)
```

```{r, warning=FALSE, message=FALSE}
hist(res_sim2$scaledResiduals, freq = FALSE, ylab = "[Residuals]", xlab = "Scaled Residuals", main = NULL)
```

**ANOVA**

```{r, warning=FALSE, message=FALSE}
Anova(m2)
```

**Post-hoc test - Mean comparisons**

```{r, warning=FALSE, message=FALSE}
means2 <- emmeans(m2, ~fungicide, type = "response")
cld(means2, Letters = letters)
```

### What if we made the wrong assumption for severity?

```{r, warning=FALSE, message=FALSE}
m3 <- glmmTMB(severity_o ~ fungicide + (1|block), family = Gamma(link = "log"), data = d2)
res_sim3 <- simulateResiduals(m3, plot = TRUE)
```

## Example 3 - Discrete response - ?

## Hierarchical models

...
